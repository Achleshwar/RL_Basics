{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4kQxedfO6beQ"
   },
   "outputs": [],
   "source": [
    "from tensorboardX import SummaryWriter\n",
    "import gym \n",
    "from collections import namedtuple\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LZa5-eUk9ZKu"
   },
   "source": [
    "Our model's core is a one-hidden-layer\n",
    "neural network, with ReLU and 128 hidden neurons (which is absolutely\n",
    "arbitrary). \n",
    "We define constants at the top of the file and they include the count of neurons in\n",
    "the hidden layer, the count of episodes we play on every iteration (16), and the\n",
    "percentile of episodes' total rewards that we use for elite episode filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Utu8vr0Z8bQo"
   },
   "outputs": [],
   "source": [
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 16\n",
    "PERCENTILE = 70"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0QEeaA7H9jAj"
   },
   "source": [
    "Our network takes a single observation from the\n",
    "environment as an input vector and outputs a number for every action we can perform. The output from the network is a probability distribution over actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1VfyzCuy9Mue"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, obs_size, hidden_size, n_actions):\n",
    "        super(Net, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "                    nn.Linear(obs_size, hidden_size),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Linear(hidden_size, n_actions)\n",
    "                    )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aAMZ2lsS_Onp"
   },
   "source": [
    "Now we will define two helper classes that are named tuples from the\n",
    "collections package in the standard library:\n",
    "\n",
    "*   EpisodeStep: This will be used to represent one single step that our agent made in the episode, and it stores the observation from the environment and what action the agent completed. We'll use episode steps from elite episodes as training data.\n",
    "*   Episode: This is a single episode stored as total undiscounted reward and a collection of EpisodeStep.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONyoaNxc--sL"
   },
   "outputs": [],
   "source": [
    "EpisodeStep = namedtuple('EpisodeStep', field_names= ['observation', 'action'])\n",
    "Episode = namedtuple('Episode', field_names= ['reward', 'steps'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TirWyfhTAGrw"
   },
   "source": [
    "The succeding function accepts the environment (the Env class instance from the\n",
    "Gym library), our neural network, and the count of episodes it should generate\n",
    "on every iteration. The batch variable will be used to accumulate our batch\n",
    "(which is a list of the Episode instances). We also declare a reward counter for\n",
    "the current episode and its list of steps (the EpisodeStep objects). Then we reset\n",
    "our environment to obtain the first observation and create a softmax layer, which\n",
    "will be used to convert the network's output to a probability distribution of\n",
    "actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vF6dTiSN_0Xp"
   },
   "outputs": [],
   "source": [
    "def iterate_batches(env, net, batch_size):\n",
    "    batch = []\n",
    "    episode_reward = 0.0\n",
    "    episode_steps = []\n",
    "    obs = env.reset()\n",
    "    sm = nn.Softmax(dim=1)\n",
    "    \"\"\"At every iteration, we convert our current observation to a PyTorch tensor and\n",
    "     pass it to the network to obtain action probabilities. There are several things to\n",
    "     note here:\n",
    "     1. All nn.Module instances in PyTorch expect a batch of data items and the\n",
    "        same is true for our network, so we convert our observation (which is a\n",
    "        vector of four numbers in CartPole) into a tensor of size 1 Ã— 4 (to achieve\n",
    "        this we pass an observation in a single-element list).\n",
    "     2. As we haven't used nonlinearity at the output of our network, it outputs raw\n",
    "        action scores, which we need to feed through the softmax function.\n",
    "     3. Both our network and the softmax layer return tensors which track\n",
    "        gradients, so we need to unpack this by accessing the tensor.data field\n",
    "        and then converting the tensor into a NumPy array.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        obs_v = torch.FloatTensor([obs])\n",
    "        action_probs_v = sm(net(obs_v))\n",
    "        action_probs = action_probs_v.data.numpy()[0]\n",
    "        \"\"\"Now that we have the probability distribution of actions, we can use this\n",
    "           distribution to obtain the actual action for the current step by sampling this\n",
    "           distribution using NumPy's function, random.choice().\n",
    "        \"\"\"\n",
    "        action = np.random.choice(len(action_probs), p = action_probs)\n",
    "        \"\"\"We pass this action to env to get our next observation and reward.\"\"\"\n",
    "        next_obs, reward, is_done, _ = env.step(action)\n",
    "        \"\"\"Reward is added to the current episode's total reward, and our list of episode\n",
    "           steps is also extended with an (observation, action) pair.\n",
    "        \"\"\"\n",
    "        episode_reward += reward\n",
    "        episode_steps.append(EpisodeStep(observation = obs, action = action))\n",
    "        \"\"\"This is how we handle the situation when the current episode is over (in the case\n",
    "          of CartPole, the episode ends when the stick has fallen down despite our efforts).\n",
    "          We append the finalized episode to the batch, saving the total reward (as the\n",
    "          episode has been completed and we've accumulated all reward) and steps we've\n",
    "          taken. Then we reset our total reward accumulator and clean the list of steps.\n",
    "          After that, we reset our environment to start over.\n",
    "\n",
    "          In case our batch has reached the desired count of episodes, we return it to the\n",
    "          caller for processing, using yield. Our function is a generator, so every time the\n",
    "          yield operator is executed, the control is transferred to the outer iteration loop\n",
    "          and then continues after the yield line\n",
    "        \"\"\"\n",
    "        if is_done:\n",
    "            batch.append(Episode(reward = episode_reward, steps = episode_steps))\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = []\n",
    "            next_obs = env.reset()\n",
    "            if len(batch) == batch_size:\n",
    "                yield batch\n",
    "                batch = []\n",
    "        obs = next_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2IT9nV3ZGQCx"
   },
   "source": [
    "This function is at the core of the cross-entropy method: \n",
    "\n",
    "From the given batch of\n",
    "episodes and percentile value, it calculates a boundary reward, which is used to\n",
    "filter elite episodes to train on. To obtain the boundary reward, we're using\n",
    "NumPy's percentile function, which from the list of values and the desired\n",
    "percentile, calculates the percentile's value. Then we will calculate mean reward,\n",
    "which is used only for monitoring.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CjEhoyj_GILu"
   },
   "outputs": [],
   "source": [
    "def filter_batch(batch, percentile):\n",
    "    rewards = list(map(lambda s: s.reward, batch))\n",
    "    reward_bound = np.percentile(rewards, percentile)\n",
    "    reward_mean = float(np.mean(rewards))\n",
    "\n",
    "    \"\"\"Next, we will filter off our episodes. For every episode in the batch, we will\n",
    "       check that the episode has a higher total reward than our boundary and if it has,\n",
    "       we will populate lists of observations and actions that we will train on.\n",
    "    \"\"\"\n",
    "    train_obs = []\n",
    "    train_act = []\n",
    "    for example in batch:\n",
    "        if example.reward < reward_bound:\n",
    "            continue\n",
    "        train_obs.extend(map(lambda step : step.observation, example.steps))\n",
    "        train_act.extend(map(lambda step : step.action, example.steps))\n",
    "\n",
    "    \"\"\"As the final step of the function, we will convert our observations and actions\n",
    "       from elite episodes into tensors, and return a tuple of four: observations, actions,\n",
    "       the boundary of reward, and the mean reward.\n",
    "    \"\"\"\n",
    "    train_obs_v = torch.FloatTensor(train_obs)\n",
    "    train_act_v = torch.LongTensor(train_act)\n",
    "    return train_obs_v, train_act_v, reward_bound, reward_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oCON22biJ01O"
   },
   "source": [
    "Now, we will write the final chunk of code that glues everything together and mostly consists of the training loop.\n",
    "\n",
    "\n",
    "In the training loop, we will iterate our batches (which are a list of Episode\n",
    "objects), then we perform filtering of the elite episodes using the filter_batch\n",
    "function. The result is variables of observations and taken actions, the reward\n",
    "boundary used for filtering and the mean reward. After that, we zero gradients of\n",
    "our network and pass observations to the network, obtaining its action scores.\n",
    "These scores are passed to the objective function, which calculates cross-entropy\n",
    "between the network output and the actions that the agent took. The idea of this\n",
    "is to reinforce our network to carry out those \"elite\" actions which have led to\n",
    "good rewards. Then, we will calculate gradients on the loss and ask the\n",
    "optimizer to adjust our network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iMZ1c1ejJg9g"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: loss=0.684, reward_mean=28.9, reward_bound=30.0\n",
      "1: loss=0.670, reward_mean=31.5, reward_bound=36.5\n",
      "2: loss=0.674, reward_mean=34.0, reward_bound=38.5\n",
      "3: loss=0.634, reward_mean=38.2, reward_bound=46.0\n",
      "4: loss=0.643, reward_mean=38.9, reward_bound=46.0\n",
      "5: loss=0.636, reward_mean=33.8, reward_bound=32.0\n",
      "6: loss=0.630, reward_mean=43.1, reward_bound=46.5\n",
      "7: loss=0.613, reward_mean=53.1, reward_bound=63.0\n",
      "8: loss=0.616, reward_mean=60.6, reward_bound=77.5\n",
      "9: loss=0.600, reward_mean=86.1, reward_bound=93.0\n",
      "10: loss=0.604, reward_mean=87.1, reward_bound=118.0\n",
      "11: loss=0.596, reward_mean=92.2, reward_bound=133.0\n",
      "12: loss=0.577, reward_mean=90.6, reward_bound=112.5\n",
      "13: loss=0.578, reward_mean=79.1, reward_bound=91.5\n",
      "14: loss=0.570, reward_mean=106.1, reward_bound=130.5\n",
      "15: loss=0.571, reward_mean=120.5, reward_bound=134.5\n",
      "16: loss=0.569, reward_mean=126.9, reward_bound=142.5\n",
      "17: loss=0.562, reward_mean=155.4, reward_bound=193.5\n",
      "18: loss=0.548, reward_mean=151.9, reward_bound=196.0\n",
      "19: loss=0.549, reward_mean=141.9, reward_bound=176.5\n",
      "20: loss=0.545, reward_mean=145.1, reward_bound=166.0\n",
      "21: loss=0.535, reward_mean=157.7, reward_bound=196.5\n",
      "22: loss=0.524, reward_mean=175.6, reward_bound=200.0\n",
      "23: loss=0.535, reward_mean=137.6, reward_bound=169.0\n",
      "24: loss=0.546, reward_mean=181.4, reward_bound=200.0\n",
      "25: loss=0.528, reward_mean=174.3, reward_bound=200.0\n",
      "26: loss=0.521, reward_mean=193.4, reward_bound=200.0\n",
      "27: loss=0.525, reward_mean=189.1, reward_bound=200.0\n",
      "28: loss=0.517, reward_mean=191.9, reward_bound=200.0\n",
      "29: loss=0.516, reward_mean=199.2, reward_bound=200.0\n",
      "Solved!\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.wrappers.Monitor(env, directory='mon', force=True)\n",
    "obs_size = env.observation_space.shape[0]\n",
    "n_actions = env.action_space.n\n",
    "\n",
    "net = Net(obs_size, HIDDEN_SIZE, n_actions)\n",
    "objective = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(params=net.parameters(), lr=0.01)\n",
    "writer = SummaryWriter(comment=\"-cartpole\")\n",
    "\n",
    "for iter_no, batch in enumerate(iterate_batches(env, net, BATCH_SIZE)):\n",
    "    obs_v, act_v, reward_b, reward_m = filter_batch(batch, PERCENTILE)\n",
    "    optimizer.zero_grad()\n",
    "    action_scores_v = net(obs_v)\n",
    "    loss_v = objective(action_scores_v, act_v)\n",
    "    loss_v.backward()\n",
    "    optimizer.step()\n",
    "    print(\"%d: loss=%.3f, reward_mean=%.1f, reward_bound=%.1f\" % (\n",
    "                iter_no, loss_v.item(), reward_m, reward_b))\n",
    "    writer.add_scalar('loss', loss_v.item(), iter_no)\n",
    "    writer.add_scalar('reward_bound', reward_b, iter_no)\n",
    "    writer.add_scalar('reward_mean', reward_m, iter_no)\n",
    "    if reward_m > 199:\n",
    "        print('Solved!')\n",
    "        break\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bSRXu_7OKf8j"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Cross-Entropy Cartpole.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
